{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                             precision_score, recall_score, f1_score)\n",
        "import time\n",
        "\n",
        "# Setting the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "#Loading the Data\n",
        "data = 'train_set.csv'\n",
        "df = pd.read_csv(data)\n",
        "\n",
        "# Separate features and target\n",
        "X_train = df.drop(columns=['fetal_health'])\n",
        "y_train = df['fetal_health']\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Class distribution:\\n\", y_train.value_counts())\n",
        "\n",
        "#Encoding the outcome variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "classes = label_encoder.classes_\n",
        "\n",
        "print(f\"\\nEncoded classes: {classes}\")\n",
        "print(f\"Encoded labels: {np.unique(y_train_encoded)}\")\n",
        "\n",
        "#Cross-Validation\n",
        "cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "#Let's Define the different scoring metrics for eval\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision_weighted': 'precision_weighted',\n",
        "    'recall_weighted': 'recall_weighted',\n",
        "    'f1_weighted': 'f1_weighted'\n",
        "}\n",
        "\n",
        "#Gradient Boosting Model\n",
        "#Creating pipeline with preprocessing\n",
        "gb_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('gb', GradientBoostingClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "#Defining hyperparameter grid\n",
        "param_grid_boost = {\n",
        "    'gb__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'gb__max_depth': [3, 5, 7],\n",
        "    'gb__n_estimators': [100, 200]   # added valid parameter\n",
        "}\n",
        "\n",
        "gb_start_time = time.time()\n",
        "\n",
        "#Initializing GridSearchCV\n",
        "grid_search_gb = GridSearchCV(\n",
        "    estimator=gb_pipeline,\n",
        "    param_grid=param_grid_boost,\n",
        "    cv=cv_splitter,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "grid_search_gb.fit(X_train, y_train_encoded)\n",
        "gb_end_time = time.time()\n",
        "print(f\"Gradient Boosting fitting time: {(gb_end_time - gb_start_time)/60:.2f} minutes\")\n",
        "\n",
        "#Evaluation\n",
        "for param, value in grid_search_gb.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "print(f\"\\nBest cross-validation accuracy: {grid_search_gb.best_score_:.4f}\")\n",
        "\n",
        "#Evaluating with Multiple Metrics\n",
        "best_gb_model = grid_search_gb.best_estimator_\n",
        "\n",
        "gb_cv_results = cross_validate(best_gb_model, X_train, y_train_encoded,\n",
        "                                cv=cv_splitter, scoring=scoring)\n",
        "\n",
        "print(f\"Accuracy:  {gb_cv_results['test_accuracy'].mean():.4f}\" )\n",
        "print(f\"Precision: {gb_cv_results['test_precision_weighted'].mean():.4f}\" )\n",
        "print(f\"Recall:    {gb_cv_results['test_recall_weighted'].mean():.4f}\" )\n",
        "print(f\"F1-Score:  {gb_cv_results['test_f1_weighted'].mean():.4f}\" )\n",
        "\n",
        "best_gb_model.fit(X_train, y_train_encoded)\n",
        "y_pred_gb = best_gb_model.predict(X_train)\n",
        "gb_cm = confusion_matrix(y_train_encoded, y_pred_gb)\n",
        "print(gb_cm)\n",
        "\n",
        "print(classification_report(y_train_encoded, y_pred_gb, target_names=[str(c) for c in classes]))\n",
        "\n",
        "#Feature IMPORTANCE\n",
        "features = X_train.columns\n",
        "feature_importance_gb = best_gb_model.named_steps['gb'].feature_importances_   # FIXED\n",
        "\n",
        "# Create dataframe\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': feature_importance_gb\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance_df.head(10).to_string(index=False))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'][:10],\n",
        "         feature_importance_df['Importance'][:10],\n",
        "         color='#2ecc71')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 10 Feature Importances - Gradient Boosting')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bSPGupraJpJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Neural Network\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Setting the random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#Loading the Data\n",
        "df = pd.read_csv(\"train_set.csv\")\n",
        "X = df.drop(columns=['fetal_health'])\n",
        "y = df['fetal_health'] - 1   # convert classes to 0-indexed\n",
        "\n",
        "print(f\"Training data shape: {X.shape}\")\n",
        "print(f\"Classes: {np.unique(y.values)}\")\n",
        "\n",
        "input_size = X.shape[1]\n",
        "output_size = len(np.unique(y))\n",
        "k = 5\n",
        "\n",
        "# DEFINE NEURAL NETWORK\n",
        "class NNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NNClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# HYPERPARAMETER CONFIGURATIONS\n",
        "\n",
        "configs = [\n",
        "    {'hidden_size': 32, 'lr': 0.001, 'n_epochs': 60, 'batch_size': 32},\n",
        "    {'hidden_size': 64, 'lr': 0.001, 'n_epochs': 60, 'batch_size': 32},\n",
        "    {'hidden_size': 64, 'lr': 0.01, 'n_epochs': 60, 'batch_size': 64},\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "# TEST EACH CONFIGURATION\n",
        "for config_idx, config in enumerate(configs):\n",
        "    print(f\"CONFIGURATION {config_idx+1}/{len(configs)}\")\n",
        "    print(f\"Hidden: {config['hidden_size']}, LR: {config['lr']}, Batch: {config['batch_size']}\")\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    fold_acc = []\n",
        "    fold_prec = []\n",
        "    fold_rec = []\n",
        "    fold_f1 = []\n",
        "\n",
        "    # Cross Validation Loop\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        print(f\"\\n  Fold {fold+1}/{k}\")\n",
        "\n",
        "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # Scaling X only\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_fold)\n",
        "        X_val_scaled = scaler.transform(X_val_fold)\n",
        "\n",
        "        # Converting into Tensors\n",
        "        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train_fold.values, dtype=torch.long)  # BUG FIX: was y_train\n",
        "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "        y_val_tensor = torch.tensor(y_val_fold.values, dtype=torch.long)\n",
        "\n",
        "        train_ds = TensorDataset(X_train_tensor, y_train_tensor)  # BUG FIX: was train_d\n",
        "        train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)  # BUG FIX: was train_ds\n",
        "\n",
        "        model = NNClassifier(input_size, config['hidden_size'], output_size)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
        "\n",
        "        # Training Loop\n",
        "        for epoch in range(config['n_epochs']):\n",
        "            model.train()\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for x_batch, y_batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                preds = model(x_batch)\n",
        "                loss = criterion(preds, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            if (epoch+1) % 20 == 0:\n",
        "                print(f\"    Epoch {epoch+1}: Loss = {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        # Model Evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds_logits = model(X_val_tensor)  # BUG FIX: was torch.model(X_test_tensor)\n",
        "            _, val_preds = torch.max(val_preds_logits, 1)  # BUG FIX: was vals_preds\n",
        "\n",
        "        # Metrics\n",
        "        accuracy = accuracy_score(y_val_tensor.numpy(), val_preds.numpy())  # BUG FIX: was y_test_tensor, test_preds\n",
        "        precision = precision_score(y_val_tensor.numpy(), val_preds.numpy(), average='weighted', zero_division=0)  # BUG FIX: added average\n",
        "        recall = recall_score(y_val_tensor.numpy(), val_preds.numpy(), average='weighted', zero_division=0)  # BUG FIX: added average\n",
        "        f1 = f1_score(y_val_tensor.numpy(), val_preds.numpy(), average='weighted', zero_division=0)  # BUG FIX: added average\n",
        "\n",
        "        # Store metrics\n",
        "        fold_acc.append(accuracy)\n",
        "        fold_prec.append(precision)\n",
        "        fold_rec.append(recall)\n",
        "        fold_f1.append(f1)\n",
        "\n",
        "        print(f\"    Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # Calculate averages for this configuration\n",
        "    mean_acc = np.mean(fold_acc)\n",
        "    std_acc = np.std(fold_acc)\n",
        "    mean_prec = np.mean(fold_prec)\n",
        "    mean_rec = np.mean(fold_rec)\n",
        "    mean_f1 = np.mean(fold_f1)\n",
        "\n",
        "    results.append({\n",
        "        'config': config_idx + 1,\n",
        "        'params': config,\n",
        "        'accuracy': mean_acc,\n",
        "        'accuracy_std': std_acc,\n",
        "        'precision': mean_prec,\n",
        "        'recall': mean_rec,\n",
        "        'f1': mean_f1\n",
        "    })\n",
        "\n",
        "    print(f\"\\n  Config {config_idx+1} Average Results:\")\n",
        "    print(f\"    Accuracy:  {mean_acc:.4f} (+/- {std_acc:.4f})\")\n",
        "    print(f\"    Precision: {mean_prec:.4f}\")\n",
        "    print(f\"    Recall:    {mean_rec:.4f}\")\n",
        "    print(f\"    F1-Score:  {mean_f1:.4f}\")\n",
        "\n",
        "# BEST CONFIGURATION\n",
        "\n",
        "print(\"HYPERPARAMETER TUNING RESULTS\")\n",
        "\n",
        "results_df = pd.DataFrame([{\n",
        "    'Config': r['config'],\n",
        "    'Hidden Size': r['params']['hidden_size'],\n",
        "    'Learning Rate': r['params']['lr'],\n",
        "    'Batch Size': r['params']['batch_size'],\n",
        "    'Accuracy': f\"{r['accuracy']:.4f} Â± {r['accuracy_std']:.4f}\",\n",
        "    'Precision': f\"{r['precision']:.4f}\",\n",
        "    'Recall': f\"{r['recall']:.4f}\",\n",
        "    'F1-Score': f\"{r['f1']:.4f}\"\n",
        "} for r in results])\n",
        "\n",
        "print(\"\\n\", results_df.to_string(index=False))\n",
        "\n",
        "best_idx = np.argmax([r['accuracy'] for r in results])\n",
        "best_config = results[best_idx]\n",
        "\n",
        "print(\"BEST CONFIGURATION\")\n",
        "print(f\"Configuration {best_config['config']}:\")\n",
        "print(f\"Hidden Size: {best_config['params']['hidden_size']}\")\n",
        "print(f\"Learning Rate: {best_config['params']['lr']}\")\n",
        "print(f\"Batch Size: {best_config['params']['batch_size']}\")\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"Accuracy:  {best_config['accuracy']:.4f} (+/- {best_config['accuracy_std']:.4f})\")\n",
        "print(f\"Precision: {best_config['precision']:.4f}\")\n",
        "print(f\"Recall:    {best_config['recall']:.4f}\")\n",
        "print(f\"F1-Score:  {best_config['f1']:.4f}\")\n",
        "\n",
        "# TRAIN FINAL MODEL\n",
        "print(\"TRAINING FINAL MODEL ON ENTIRE TRAINING SET\")\n",
        "\n",
        "# Scale entire dataset\n",
        "scaler_final = StandardScaler()\n",
        "X_scaled_final = scaler_final.fit_transform(X)\n",
        "\n",
        "# Convert to tensors\n",
        "X_tensor_final = torch.tensor(X_scaled_final, dtype=torch.float32)\n",
        "y_tensor_final = torch.tensor(y.values, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_ds_final = TensorDataset(X_tensor_final, y_tensor_final)\n",
        "train_loader_final = DataLoader(train_ds_final, batch_size=best_config['params']['batch_size'], shuffle=True)\n",
        "\n",
        "# Initialize final model with best hyperparameters\n",
        "final_model = NNClassifier(input_size, best_config['params']['hidden_size'], output_size)\n",
        "criterion_final = nn.CrossEntropyLoss()\n",
        "optimizer_final = optim.Adam(final_model.parameters(), lr=best_config['params']['lr'])\n",
        "\n",
        "# Train final model\n",
        "for epoch in range(best_config['params']['n_epochs']):\n",
        "    final_model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for x_batch, y_batch in train_loader_final:\n",
        "        optimizer_final.zero_grad()\n",
        "        preds = final_model(x_batch)\n",
        "        loss = criterion_final(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer_final.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Loss = {epoch_loss/len(train_loader_final):.4f}\")\n",
        "\n",
        "# VISUALIZATIONS\n",
        "\n",
        "# Evaluate final model for confusion matrix\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    final_preds_logits = final_model(X_tensor_final)\n",
        "    _, final_preds = torch.max(final_preds_logits, 1)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_tensor_final.numpy(), final_preds.numpy())\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', cbar=True)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Neural Network - Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_tensor_final.numpy(), final_preds.numpy()))\n",
        "\n",
        "# Plot configuration comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "configs_names = [f\"Config {r['config']}\" for r in results]\n",
        "accuracies = [r['accuracy'] for r in results]\n",
        "stds = [r['accuracy_std'] for r in results]\n",
        "\n",
        "plt.bar(configs_names, accuracies, yerr=stds, capsize=5, alpha=0.7, color='#9b59b6')\n",
        "plt.xlabel('Configuration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Neural Network - Hyperparameter Comparison')\n",
        "plt.ylim([0, 1])\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G-ZGu4HCCNcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}